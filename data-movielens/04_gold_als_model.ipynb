{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83f5600-ae8a-4b39-97b5-45349e5aeb22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+-------------------+-----------+\n|userId|movieId|rating|timestamp|       timestamp_ts|rating_date|\n+------+-------+------+---------+-------------------+-----------+\n|   236|   2718|   3.0|943015211|1999-11-19 12:40:11| 1999-11-19|\n|   236|   2542|   5.0|943015062|1999-11-19 12:37:42| 1999-11-19|\n|   236|   2459|   4.0|943013896|1999-11-19 12:18:16| 1999-11-19|\n|   236|   2580|   5.0|943015701|1999-11-19 12:48:21| 1999-11-19|\n|   236|   1350|   4.0|943013896|1999-11-19 12:18:16| 1999-11-19|\n+------+-------+------+---------+-------------------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(\"gold_fact_ratings\")\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1963dcfb-d9ba-411a-b0e3-509f92bb0983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: float (nullable = true)\n |-- timestamp: string (nullable = true)\n |-- timestamp_ts: timestamp (nullable = true)\n |-- rating_date: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7989cf8-cb41-44e1-ae74-7f5a45197443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6546703a-467c-410b-b304-2a77f1f36d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e5b1ac-eaf6-45e5-85ad-c4bacbed26ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative=True,\n",
    "    implicitPrefs=False,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    rank=10,\n",
    "    maxIter=10,\n",
    "    regParam=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d70e85-b484-4cd4-b339-6b9113796598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = als.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8228611-ce2e-4557-bad9-6adabdbda6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8123190612289932\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481ff572-6766-4194-b3a8-52c53d0dbc4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users = df.select(\"userId\").distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65bb6bf4-b459-4eb5-9810-fdf24fca47d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_recs = model.recommendForUserSubset(users, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1b8326-371d-4190-be73-adc5ce89f2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users = df.select(\"userId\").distinct()\n",
    "movies = df.select(\"movieId\").distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bcb25ae-837c-41ce-9ab0-12c1ac467fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_movie_pairs = users.crossJoin(movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "300ab322-dcbe-4303-a70e-e15183d57669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(user_movie_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83874be9-59d7-4789-979e-2874f9a0587d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "window_spec = Window.partitionBy(\"userId\").orderBy(F.desc(\"prediction\"))\n",
    "\n",
    "top_recs = predictions \\\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "    .filter(F.col(\"rank\") <= 10) \\\n",
    "    .select(\"userId\", \"movieId\", F.col(\"prediction\").alias(\"predicted_rating\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "450a0c10-70ce-43b0-957c-40ff969946fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------------+---------------------------------------------------------+\n|movieId|userId|predicted_rating|title                                                    |\n+-------+------+----------------+---------------------------------------------------------+\n|170355 |1     |5.8688707       |Mulholland Dr. (1999)                                    |\n|3379   |1     |5.8688707       |On the Beach (1959)                                      |\n|33649  |1     |5.821521        |Saving Face (2004)                                       |\n|27523  |1     |5.6181192       |My Sassy Girl (Yeopgijeogin geunyeo) (2001)              |\n|171495 |1     |5.581305        |Cosmos                                                   |\n|132333 |1     |5.533605        |Seve (2014)                                              |\n|72171  |1     |5.468348        |Black Dynamite (2009)                                    |\n|86237  |1     |5.468348        |Connections (1978)                                       |\n|117531 |1     |5.468348        |Watermark (2014)                                         |\n|26073  |1     |5.468348        |Human Condition III, The (Ningen no joken III) (1961)    |\n|33649  |2     |4.9484396       |Saving Face (2004)                                       |\n|131724 |2     |4.886967        |The Jinx: The Life and Deaths of Robert Durst (2015)     |\n|51931  |2     |4.857955        |Reign Over Me (2007)                                     |\n|174053 |2     |4.7491803       |Black Mirror: White Christmas (2014)                     |\n|89904  |2     |4.7361403       |The Artist (2011)                                        |\n|147374 |2     |4.658734        |Doctor Who: The Doctor, the Widow and the Wardrobe (2011)|\n|106642 |2     |4.6560636       |Day of the Doctor, The (2013)                            |\n|106100 |2     |4.6312585       |Dallas Buyers Club (2013)                                |\n|96821  |2     |4.597445        |Perks of Being a Wallflower, The (2012)                  |\n|80586  |2     |4.5689173       |Flipped (2010)                                           |\n+-------+------+----------------+---------------------------------------------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_movies = spark.table(\"silver_dim_movies_enriched\")\n",
    "\n",
    "recs_with_titles = top_recs.join(\n",
    "    df_movies.select(\"movieId\", \"title\"),\n",
    "    \"movieId\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "recs_with_titles.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418f5dba-89ce-4493-9d04-ab279604a561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "recs_with_titles.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"gold_user_recommendations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3e28e4-12e5-4fdd-9972-ab13b58df07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_fact_ratings  |  MANAGED  |  False\ngold_user_recommendations  |  MANAGED  |  False\n"
     ]
    }
   ],
   "source": [
    "tables = spark.catalog.listTables()\n",
    "for t in tables:\n",
    "    if t.name.startswith(\"gold_\"):\n",
    "        print(f\"{t.name}  |  {t.tableType}  |  {t.isTemporary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae3f33f-d965-40eb-92d8-d462f3b3695a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+-------------------+-----------+\n|userId|movieId|rating|timestamp|       timestamp_ts|rating_date|\n+------+-------+------+---------+-------------------+-----------+\n|   236|   2718|   3.0|943015211|1999-11-19 12:40:11| 1999-11-19|\n|   236|   2542|   5.0|943015062|1999-11-19 12:37:42| 1999-11-19|\n|   236|   2459|   4.0|943013896|1999-11-19 12:18:16| 1999-11-19|\n|   236|   2580|   5.0|943015701|1999-11-19 12:48:21| 1999-11-19|\n|   236|   1350|   4.0|943013896|1999-11-19 12:18:16| 1999-11-19|\n+------+-------+------+---------+-------------------+-----------+\nonly showing top 5 rows\n+-------+------+----------------+-------------------------------------------+\n|movieId|userId|predicted_rating|title                                      |\n+-------+------+----------------+-------------------------------------------+\n|170355 |1     |5.8688707       |Mulholland Dr. (1999)                      |\n|3379   |1     |5.8688707       |On the Beach (1959)                        |\n|33649  |1     |5.821521        |Saving Face (2004)                         |\n|27523  |1     |5.6181192       |My Sassy Girl (Yeopgijeogin geunyeo) (2001)|\n|171495 |1     |5.581305        |Cosmos                                     |\n+-------+------+----------------+-------------------------------------------+\nonly showing top 5 rows\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fact ratings\n",
    "spark.table(\"gold_fact_ratings\").show(5)\n",
    "spark.table(\"gold_fact_ratings\").count()\n",
    "\n",
    "# Recommendations\n",
    "spark.table(\"gold_user_recommendations\").show(5, truncate=False)\n",
    "spark.table(\"gold_user_recommendations\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98c809c-e1c0-44ea-9dc1-e8a90df1582e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+\n|movieId|avg_rating|num_ratings|\n+-------+----------+-----------+\n|   6442|       5.0|          2|\n|   6818|       5.0|          2|\n|     53|       5.0|          2|\n|   1151|       5.0|          2|\n|     99|       5.0|          2|\n|   3473|       5.0|          2|\n|  78836|       5.0|          2|\n|  96608|       5.0|          1|\n| 139640|       5.0|          1|\n|   3303|       5.0|          1|\n+-------+----------+-----------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.table(\"gold_fact_ratings\") \\\n",
    "    .groupBy(\"movieId\") \\\n",
    "    .agg(\n",
    "        F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "        F.count(\"rating\").alias(\"num_ratings\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"avg_rating\"), F.desc(\"num_ratings\")) \\\n",
    "    .show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db45d76-04be-4b3d-adc9-40c235b002ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"rank\", model.rank)\n",
    "    mlflow.log_param(\"regParam\", getattr(model, 'regParam', 'not available'))\n",
    "    mlflow.log_metric(\"rmse\", 0.8123190612289932)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b3dc2b-e2f6-473b-8807-46f597337c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-02-17 13:52:54.295\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2026-02-17T13:52:54.295139972+00:00\\\", grpc_status:13, grpc_message:\\\"[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2099\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"143\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"204\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"176\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"313\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"285\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"177\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"696\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2026-02-17 13:52:54.295\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2026-02-17T13:52:54.295139972+00:00\\\", grpc_status:13, grpc_message:\\\"[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2099\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"143\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"204\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"176\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"313\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"285\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"177\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"696\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2026-02-17 13:52:54.295\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2026-02-17T13:52:54.295139972+00:00\\\", grpc_status:13, grpc_message:\\\"[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"2099\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"143\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"204\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"176\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"313\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"285\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"177\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"696\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5385075738654564>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrecommendation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ALSModel\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load the trained model if not already in memory\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m ALSModel\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/path/to/gold_als_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# or use the DataFrame you trained\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Generate top 10 recommendations per user\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m top_recs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mrecommendForAllUsers(\u001B[38;5;241m10\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/util.py:929\u001B[0m, in \u001B[0;36mMLReadable.load\u001B[0;34m(cls, path)\u001B[0m\n",
       "\u001B[1;32m    926\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n",
       "\u001B[1;32m    927\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mcls\u001B[39m, path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RL:\n",
       "\u001B[1;32m    928\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 929\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mload(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/connect/readwrite.py:228\u001B[0m, in \u001B[0;36mRemoteMLReader.load\u001B[0;34m(self, path)\u001B[0m\n",
       "\u001B[1;32m    225\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mgetActiveSession()\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m session \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 228\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m RemoteMLReader\u001B[38;5;241m.\u001B[39mloadInstance(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clazz, path, session)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/connect/readwrite.py:274\u001B[0m, in \u001B[0;36mRemoteMLReader.loadInstance\u001B[0;34m(clazz, path, session)\u001B[0m\n",
       "\u001B[1;32m    268\u001B[0m command \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mCommand()\n",
       "\u001B[1;32m    269\u001B[0m command\u001B[38;5;241m.\u001B[39mml_command\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mCopyFrom(\n",
       "\u001B[1;32m    270\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mMlCommand\u001B[38;5;241m.\u001B[39mRead(\n",
       "\u001B[1;32m    271\u001B[0m         operator\u001B[38;5;241m=\u001B[39mpb2\u001B[38;5;241m.\u001B[39mMlOperator(name\u001B[38;5;241m=\u001B[39mjava_qualified_class_name, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39mml_type), path\u001B[38;5;241m=\u001B[39mpath\n",
       "\u001B[1;32m    272\u001B[0m     )\n",
       "\u001B[1;32m    273\u001B[0m )\n",
       "\u001B[0;32m--> 274\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(command)\n",
       "\u001B[1;32m    275\u001B[0m result \u001B[38;5;241m=\u001B[39m deserialize(properties)\n",
       "\u001B[1;32m    277\u001B[0m \u001B[38;5;66;03m# Get the python type\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1589\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1587\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1588\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_command_in_plan(req\u001B[38;5;241m.\u001B[39mplan, command)\n",
       "\u001B[0;32m-> 1589\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1590\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1591\u001B[0m )\n",
       "\u001B[1;32m   1592\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1593\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2139\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2136\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2139\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2140\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2141\u001B[0m     ):\n",
       "\u001B[1;32m   2142\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2143\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2115\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2113\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2114\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2431\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2432\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2433\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2434\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2435\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2511\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2507\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2509\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2511\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2512\u001B[0m                 info,\n",
       "\u001B[1;32m   2513\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2514\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2515\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2516\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2517\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2520\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2521\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2522\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2523\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2524\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2755)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:1126)\n",
       "\tat scala.collection.immutable.List.flatMap(List.scala:294)\n",
       "\tat scala.collection.immutable.List.flatMap(List.scala:79)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:1107)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:722)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:543)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:277)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:114)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:113)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:60)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:47)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:47)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:60)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:58)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:395)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:569)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:818)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1766)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1100)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1097)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1097)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1096)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:1084)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1095)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:1094)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:560)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:559)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:604)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:486)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1279)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1279)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:164)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:70)\n",
       "\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:560)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:483)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:70)\n",
       "\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:534)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:479)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:70)\n",
       "\tat org.apache.spark.ml.util.ReadWriteUtils$.loadText(ReadWrite.scala:1093)\n",
       "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:727)\n",
       "\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:612)\n",
       "\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:606)\n",
       "\tat org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:394)\n",
       "\tat org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:394)\n",
       "\tat org.apache.spark.ml.recommendation.ALSModel$.load(ALS.scala:580)\n",
       "\tat org.apache.spark.ml.recommendation.ALSModel.load(ALS.scala:-1)\n",
       "\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\n",
       "\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat org.apache.spark.sql.connect.ml.MLUtils$.loadOperator(MLUtils.scala:422)\n",
       "\tat org.apache.spark.sql.connect.ml.MLUtils$.loadTransformer(MLUtils.scala:454)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:480)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:569)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:622)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:598)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:622)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3621)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3604)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:404)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:239)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:239)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:609)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:609)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:608)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2755)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:1126)\n\tat scala.collection.immutable.List.flatMap(List.scala:294)\n\tat scala.collection.immutable.List.flatMap(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:1107)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:543)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:277)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:114)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:113)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:60)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:60)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:395)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:569)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:818)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1766)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1100)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1096)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:1084)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1095)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:1094)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:560)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:559)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:604)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:486)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1279)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1279)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:164)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:70)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:560)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:483)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:70)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:534)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:479)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:70)\n\tat org.apache.spark.ml.util.ReadWriteUtils$.loadText(ReadWrite.scala:1093)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:727)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:612)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:606)\n\tat org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:394)\n\tat org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:394)\n\tat org.apache.spark.ml.recommendation.ALSModel$.load(ALS.scala:580)\n\tat org.apache.spark.ml.recommendation.ALSModel.load(ALS.scala:-1)\n\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\n\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:569)\n\tat org.apache.spark.sql.connect.ml.MLUtils$.loadOperator(MLUtils.scala:422)\n\tat org.apache.spark.sql.connect.ml.MLUtils$.loadTransformer(MLUtils.scala:454)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:480)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:569)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:622)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:598)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:622)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3621)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3604)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:608)"
       },
       "metadata": {
        "errorSummary": "[PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "PATH_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "42K03",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2755)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:1126)\n\tat scala.collection.immutable.List.flatMap(List.scala:294)\n\tat scala.collection.immutable.List.flatMap(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:1107)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:543)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:277)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:114)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:113)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:60)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:60)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:395)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:569)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:818)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1766)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1100)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1096)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:1084)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1095)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:1094)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:560)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:559)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:604)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:486)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1279)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1279)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:164)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:70)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:560)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:483)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:70)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:534)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:479)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:70)\n\tat org.apache.spark.ml.util.ReadWriteUtils$.loadText(ReadWrite.scala:1093)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:727)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:612)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:606)\n\tat org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:394)\n\tat org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:394)\n\tat org.apache.spark.ml.recommendation.ALSModel$.load(ALS.scala:580)\n\tat org.apache.spark.ml.recommendation.ALSModel.load(ALS.scala:-1)\n\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\n\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:569)\n\tat org.apache.spark.sql.connect.ml.MLUtils$.loadOperator(MLUtils.scala:422)\n\tat org.apache.spark.sql.connect.ml.MLUtils$.loadTransformer(MLUtils.scala:454)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:480)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:569)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:622)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:598)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:622)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3621)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3604)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:608)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5385075738654564>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrecommendation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ALSModel\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load the trained model if not already in memory\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m ALSModel\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/path/to/gold_als_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# or use the DataFrame you trained\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Generate top 10 recommendations per user\u001B[39;00m\n\u001B[1;32m      7\u001B[0m top_recs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mrecommendForAllUsers(\u001B[38;5;241m10\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/util.py:929\u001B[0m, in \u001B[0;36mMLReadable.load\u001B[0;34m(cls, path)\u001B[0m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    927\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mcls\u001B[39m, path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RL:\n\u001B[1;32m    928\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 929\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mload(path)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/connect/readwrite.py:228\u001B[0m, in \u001B[0;36mRemoteMLReader.load\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    225\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mgetActiveSession()\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m session \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 228\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m RemoteMLReader\u001B[38;5;241m.\u001B[39mloadInstance(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clazz, path, session)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/connect/readwrite.py:274\u001B[0m, in \u001B[0;36mRemoteMLReader.loadInstance\u001B[0;34m(clazz, path, session)\u001B[0m\n\u001B[1;32m    268\u001B[0m command \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mCommand()\n\u001B[1;32m    269\u001B[0m command\u001B[38;5;241m.\u001B[39mml_command\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mCopyFrom(\n\u001B[1;32m    270\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mMlCommand\u001B[38;5;241m.\u001B[39mRead(\n\u001B[1;32m    271\u001B[0m         operator\u001B[38;5;241m=\u001B[39mpb2\u001B[38;5;241m.\u001B[39mMlOperator(name\u001B[38;5;241m=\u001B[39mjava_qualified_class_name, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39mml_type), path\u001B[38;5;241m=\u001B[39mpath\n\u001B[1;32m    272\u001B[0m     )\n\u001B[1;32m    273\u001B[0m )\n\u001B[0;32m--> 274\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(command)\n\u001B[1;32m    275\u001B[0m result \u001B[38;5;241m=\u001B[39m deserialize(properties)\n\u001B[1;32m    277\u001B[0m \u001B[38;5;66;03m# Get the python type\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1589\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1587\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1588\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_command_in_plan(req\u001B[38;5;241m.\u001B[39mplan, command)\n\u001B[0;32m-> 1589\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1590\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1591\u001B[0m )\n\u001B[1;32m   1592\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1593\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2139\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2136\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2139\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2140\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2141\u001B[0m     ):\n\u001B[1;32m   2142\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2143\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2115\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2113\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2114\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2431\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2432\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2433\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2434\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2435\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2511\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2507\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2509\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2511\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2512\u001B[0m                 info,\n\u001B[1;32m   2513\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2514\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2515\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2516\u001B[0m                 status_code,\n\u001B[1;32m   2517\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2520\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2521\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2522\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2523\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2524\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/path/to/gold_als_model/metadata. SQLSTATE: 42K03\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2755)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:1126)\n\tat scala.collection.immutable.List.flatMap(List.scala:294)\n\tat scala.collection.immutable.List.flatMap(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:1107)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:543)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:277)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:114)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:113)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:60)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:60)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:395)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:569)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:818)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1766)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1100)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1096)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:1084)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1095)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:1094)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:560)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:559)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:604)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:486)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1279)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:201)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1279)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:164)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:70)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:560)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:483)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:70)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:534)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:479)\n\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:70)\n\tat org.apache.spark.ml.util.ReadWriteUtils$.loadText(ReadWrite.scala:1093)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:727)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:612)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:606)\n\tat org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:394)\n\tat org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:394)\n\tat org.apache.spark.ml.recommendation.ALSModel$.load(ALS.scala:580)\n\tat org.apache.spark.ml.recommendation.ALSModel.load(ALS.scala:-1)\n\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\n\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:569)\n\tat org.apache.spark.sql.connect.ml.MLUtils$.loadOperator(MLUtils.scala:422)\n\tat org.apache.spark.sql.connect.ml.MLUtils$.loadTransformer(MLUtils.scala:454)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:480)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:569)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:622)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:598)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:622)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3621)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3604)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:608)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "# Load the trained model if not already in memory\n",
    "model = ALSModel.load(\"dbfs:/path/to/gold_als_model\")  # or use the DataFrame you trained\n",
    "\n",
    "# Generate top 10 recommendations per user\n",
    "top_recs = model.recommendForAllUsers(10)\n",
    "\n",
    "# Optional: flatten for easier querying\n",
    "from pyspark.sql.functions import explode\n",
    "top_recs_flat = top_recs.select(\"userId\", explode(\"recommendations\").alias(\"rec\"))\n",
    "top_recs_flat = top_recs_flat.select(\n",
    "    \"userId\",\n",
    "    top_recs_flat.rec.movieId.alias(\"movieId\"),\n",
    "    top_recs_flat.rec.rating.alias(\"predicted_rating\")\n",
    ")\n",
    "\n",
    "top_recs_flat.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327e323a-2341-4d30-9940-f93a4683dee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5385075738654566>, line 6\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m data_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/movie-recommender-data-raw/gold_als_model/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m model_path \u001B[38;5;241m=\u001B[39m data_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgold_als_model\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m----> 6\u001B[0m als_model\u001B[38;5;241m.\u001B[39mwrite()\u001B[38;5;241m.\u001B[39moverwrite()\u001B[38;5;241m.\u001B[39msave(model_path)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'als_model' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'als_model' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'als_model' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOTEBOOK_USER_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KAN00",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5385075738654566>, line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m data_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/movie-recommender-data-raw/gold_als_model/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m model_path \u001B[38;5;241m=\u001B[39m data_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgold_als_model\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m als_model\u001B[38;5;241m.\u001B[39mwrite()\u001B[38;5;241m.\u001B[39moverwrite()\u001B[38;5;241m.\u001B[39msave(model_path)\n",
        "\u001B[0;31mNameError\u001B[0m: name 'als_model' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "data_path = \"/FileStore/tables/movie-recommender-data-raw/gold_als_model/\"\n",
    "\n",
    "model_path = data_path + \"gold_als_model\"\n",
    "als_model.write().overwrite().save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d18163-2351-4059-ba30-1e158d5f3db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_fact_ratings  |  MANAGED  |  False\ngold_user_recommendations  |  MANAGED  |  False\n"
     ]
    }
   ],
   "source": [
    "tables = spark.catalog.listTables()\n",
    "for t in tables:\n",
    "    if t.name.startswith(\"gold_\"):\n",
    "        print(f\"{t.name}  |  {t.tableType}  |  {t.isTemporary}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_gold_als_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}